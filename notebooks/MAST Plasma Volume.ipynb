{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85bb5bf",
   "metadata": {},
   "source": [
    "# MAST Plasma Volume Challenge\n",
    "\n",
    "This challenge asks you to infer plasma volume from frames captured by a wide-angle visible spectrum camera on the CCFE's Mega Ampere Spherical Tokamak (MAST).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is the second of three Data Science challenges for the ITER International School 2024.\n",
    "\n",
    "The animation below shows footage from a wide-angle proton bullet camera installed on MAST. This visible spectrum camera captures high frame-rate recordings showing the complete plasma cross-section on both sides of the central column. Similar cameras first captured visual recordings of ELM structures on MAST and AUG in 2007.\n",
    "\n",
    "![MAST Proton Camera Animation](../media/images/c3_proton_camera.gif)\n",
    "\n",
    "**Challenge Goal:** Develop a machine learning algorithm that predicts plasma volume from a single frame of the camera feed.\n",
    "\n",
    "This challenge introduces you to techniques for inferring a parameter from 2D image data.\n",
    "\n",
    "The MAST Data Catalog provided the data for this competition. Thanks to the curators Samuel Jackson, Nathan Cummings, Saiful Khan, and the wider MAST community for this FAIR dataset initiative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6742e0",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The image below shows the maximum plasma volume achieved for all shots in the MAST M9 campaign. These experiments were the last performed before the major upgrade to create the current MAST-U machine.\n",
    "\n",
    "![Maximum Plasma Volume](../media/images/plasma_volume.png)\n",
    "\n",
    "Maximum plasma volumes range from ~6 to ~10 cubic meters.\n",
    "\n",
    "This challenge asks you to predict the volume of a plasma geometry from camera imagery and known characteristics, a problem with practical applications in tokamak operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b68d5",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "The `./fair_mast_data/plasma_volume` directory contains all necessary files for this challenge.\n",
    "\n",
    "### Files\n",
    "- `train.nc` - Training dataset in netCDF format\n",
    "- `test.nc` - Test dataset in netCDF format\n",
    "\n",
    "### Data Structure\n",
    "- `shot_id` - Unique identifier for each shot\n",
    "- `frame` - Stack of camera frames with dimensions (shot_id, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c2836",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Both training and test datasets use the netCDF format. This self-describing format includes important metadata such as image dimensions alongside the data itself.\n",
    "\n",
    "After importing the prerequisite libraries, load the datasets using the xarray library as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.decomposition\n",
    "import sklearn.ensemble\n",
    "import sklearn.kernel_ridge\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import xarray as xr\n",
    "\n",
    "path = pathlib.Path().absolute().parent / \"fair_mast_data/plasma_volume\"\n",
    "\n",
    "try:\n",
    "    train = xr.open_dataset(path / \"train.nc\")\n",
    "    test = xr.open_dataset(path / \"test.nc\")\n",
    "except FileNotFoundError:\n",
    "    print(\"The plasma volume dataset is too large to commit to the repository.\")\n",
    "    print(\"Please download it from the FAIR-MAST data repository.\")\n",
    "    # TODO implement FAIR-MAST API here to curate the required train and test datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257f556",
   "metadata": {},
   "source": [
    "**Important:** The camera images have dimensions (shot_id, height, width). You must reshape this data to follow the (n_samples, n_features) format required by scikit-learn. After reshaping, you can split the data using standard techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbebff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.frame.values.reshape(train.sizes[\"shot_id\"], -1)\n",
    "y = train.plasma_volume\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17cda5",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "The camera frames have high dimensionality. To make the problem tractable, apply dimensionality reduction techniques. \n",
    "\n",
    "The pipeline below uses KernelPCA decomposition as a preprocessing step. This component offers several tuning hyperparameters that can affect solution accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9126615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.KernelPCA(n_components=25),\n",
    "    sklearn.linear_model.LinearRegression(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73fd086",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "With the preprocessing pipeline in place, fit the model to your training data and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e0c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model R2 0.531\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_predict = pipeline.predict(X_test)\n",
    "    R2 = sklearn.metrics.r2_score(y_test, y_predict)\n",
    "    print(f\"model R2 {R2:1.3f}\")\n",
    "except NameError:\n",
    "    print(\"Training data not available. Dataset must be loaded first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c95dc",
   "metadata": {},
   "source": [
    "## Solution Submission\n",
    "\n",
    "Prepare your solution file following the same approach as the plasma_current competition. Remember to reshape the test frames to match the (n_samples, n_features) format expected by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0473853",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = pipeline.predict(test.frame.values.reshape(test.sizes[\"shot_id\"], -1))\n",
    "solution = pd.DataFrame(\n",
    "    {\"plasma_volume\": volume}, index=pd.Index(test.shot_id, name=\"shot_id\")\n",
    ")\n",
    "solution.to_csv(path / \"linear_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's improve our model with hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'kernelpca__n_components': [25, 50, 100],\n",
    "    'kernelpca__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'linearregression__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_predict_best = best_model.predict(X_test)\n",
    "R2_best = sklearn.metrics.r2_score(y_test, y_predict_best)\n",
    "print(f\"Best model R2: {R2_best:.3f}\")\n",
    "\n",
    "# Generate final predictions\n",
    "best_volume = best_model.predict(test.frame.values.reshape(test.sizes[\"shot_id\"], -1))\n",
    "best_solution = pd.DataFrame(\n",
    "    {\"plasma_volume\": best_volume}, index=pd.Index(test.shot_id, name=\"shot_id\")\n",
    ")\n",
    "best_solution.to_csv(path / \"optimized_regression.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_challenges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
